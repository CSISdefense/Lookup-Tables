---
title: "Import FPDS delta files"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

# Whole database imports from monthly delta files to ETL.FPDSdelta

## 1) Setup

This should run very quickly, but you will need to take steps 1.A-1.C
\### 1.A) Add your USAspending-local folder if needed.

If this is your first time importing a file on this computer, copy and
paste two of the directory specifying lines and change both lines to
match your USAspending-local folder. The code chunk below looks like
this:

``` r
} else if(dir.exists("C:\\Users\\grego\\Repos\\USAspending-local\\")) {  
path\<-"C:\\Users\\grego\\Repos\\USAspending-local\\"  
```

### 1.B) Unzip the delta file and provide the directory name

**Every time**, update the deltadir variable to the month that next
needs to be imported. (Note you'll need to have already unzipped the
delta file to create this folder). The code chunk below looks like this:
`r deltadir\<-"FY(All)\_All_Contracts_Delta_20250906"`

### 1.C) Have access to the SQL login and password.

I recommend keeping it in a password manager.

```{r setup}
library(dplyr)
library(tidyr)
library(tidyverse)
library(csis360)
library(readr)
library(sqldf)
library(odbc)
library(askpass)
library(DBI)

login<-askpass("Please enter the SQL login account")
pwd<-askpass("Please enter the SQL account password")

if(dir.exists("C:\\Users\\grego\\Repositories\\USAspending-local\\")){
  path<-"C:\\Users\\grego\\Repositories\\USAspending-local\\"
} else if(dir.exists("D:\\Users\\Greg\\Repositories\\USAspending-local\\")) {
  path<-"D:\\Users\\Greg\\Repositories\\USAspending-local\\"
} else if(dir.exists("F:\\Users\\Greg\\Repositories\\USAspending-local\\")) {
  path<-"F:\\Users\\Greg\\Repositories\\USAspending-local\\"
} else if(dir.exists("C:\\Users\\grego\\Repos\\USAspending-local\\")) {
  path<-"C:\\Users\\grego\\Repos\\USAspending-local\\"
} else{
  stop("USAspending-local dir location unknown")
}
deltadir<-"FY(All)_All_Contracts_Delta_20251006"


# EXEC ETL.SP_LastModifiedDateHistory
```

## 2) Import from flatfiles to R

Running the next several line will read the unzipped delta files into r
and then saves then as RDA files. Each file except the last has 1
million rows and will take roughly 5 minutes per file in zip.

```{r import_to_r}
file.list<-list.files(file.path(path,deltadir))
file.list<-file.list[substr(file.list,nchar(file.list)-3,nchar(file.list))==".csv"]

#This should be quite fast, roughly 5 minutes per file.
#Import
for (f in 1:length(file.list)){
  print(c(deltadir,f,"Read Start", format(Sys.time(), "%c")))
  fd<-read_csv(file.path(path,deltadir,file.list[f]), guess_max = 10000000)
  print(c(deltadir,f,"Read Finish", format(Sys.time(), "%c")))
  save_file=paste0(substr(file.list[f],1,nchar(file.list)-4),".rda")
  save(fd,file=file.path(path,deltadir,save_file))
}
```

## 3) Check for Data Mismatches

This section uploads the monthly delta files to SQL server. Each file
except the last takes roughly fifteen minutes to check.

There are multiple categories of errors:

read_and_join_experiment(filecheck,"ETL_FPDSdelta_sizetype.csv",dir

-   New unrecognized column

Update [ImportAids /
missing_FPDSdelta_matches.csv](https://github.com/CSISdefense/Lookup-Tables/blob/master/ImportAids/ETL_FPDSdelta_sizetype.csv)

-   Size of character columns. Multiple columns may fail this as once
    and this is the most common form of errors.
    -   `r Error: Column length will lead to truncation`

The view code will bring up a window describing the problem columns:

``` r
View(t %\>% filter (destinationtype %in% c("varchar","nvarchar") & maxlen>varcharsize))
```

This must be fixed in at least two places:

-   Import Aids "ETL_FPDSdelta_sizetype.csv"

-   In SQL Server in the "ETL"."FPDSdelta" table

``` sql
alter table ETL.fpdsdelta
alter column [column name] varchar([new size]) 
```

-   If this is a field that carries over into contract.fpds, it will
    need to be changed there as well.

-   If this is a foreign key column, this will be a painful process
    requiring temporary disabiling of foreign and primary keys, changing
    of the type in the foreign key table, and then changing the type in
    the

-   A column does not import into the expect type.

    -   `r Conversion to number failed for [column name]`
    -   `r Number overflows decimal(19,4) for [column name]`
    -   `r Number overflows int for [column name]`
    -   `r Number overflows smallint for [column name]`
    -   `r Number overflows tinyint for [column name]`
    -   `r Conversion to date failed for [column name]`
    -   `r Conversion to bit failed for [column name]`

-   A column type in the destination table is not recognized in the code

    -   `r Unknown destination type [destination type]`


```{r Mismatch Check}
file.list<-list.files(file.path(path,deltadir))
file.list<-file.list[substr(file.list,nchar(file.list)-3,nchar(file.list))==".rda"]


for (f in 1:length(file.list)){
  load(file.path(path,deltadir,file.list[f]))
  fd<-as.data.frame(fd)

  print(c("File",file.list[f],"Error Checking Start", format(Sys.time(), "%c")))
  print(summary(factor(fd$correction_delete_ind)))
  print(summary(factor(fd$action_date_fiscal_year)))
  filecheck<-data.frame(colname=colnames(fd))
  filecheck$maxlen<-NULL
  for(c in 1:nrow(filecheck)){
    if (numbers::mod(c,50) == 0) print(c)
    filecheck$maxlen[c]<-max(sapply(fd[,filecheck$colname[c]],nchar),na.rm=TRUE)
  }
  filecheck$importtype<-sapply(fd,class)
  t<-read_and_join_experiment(filecheck,"ETL_FPDSdelta_sizetype.csv",directory="ImportAids//",skip_check_var = "varcharsize",
                              path="offline",missing_file = "output//missing_FPDSdelta_matches.csv")
  # t$varcharsize<-text_to_number(t$varcharsize)
  if(nrow(t %>% filter (destinationtype %in% c("varchar","nvarchar") & maxlen>varcharsize))>0){
    View(t %>% filter (destinationtype %in% c("varchar","nvarchar") & maxlen>varcharsize))
    stop("Column length will lead to truncation")
  }
  # Handle a misinterpretation by read_csv of NAN
  for(c in t$colname[t$importtype %in% c("numeric") & t$destinationtype %in% c("varchar","nvarchar")]){
    if(any(is.nan(fd[,c]))){
      fd[,c]<-as.character(fd[,c])
      fd[is.nan(fd[,c]),c]<-"NAN"
    }
    else{
      fd[,c]<-as.character(fd[,c])
    }
  }
  
  # Handle numerical formats
  for(c in t$colname[t$destinationtype %in% c("decimal","int","bigint","smallint","tinyint")& t$importtype=="character"]){
    converted_num<-text_to_number(fd[,c])
    if(any(!is.na(fd[,c])& 
           is.na(converted_num)))
      stop(paste("Conversion to number failed for",c))
    fd[,c]<-converted_num
    m<-max(abs(fd[,c]),na.rm=TRUE)
    if(t$destinationtype[t$colname==c] == "decimal" & m>999999999999999.9999)
      stop(paste("Number overflows decimal(19,4) for",c))
    if(t$destinationtype[t$colname==c] == "int" & m>2147483647)
      stop(paste("Number overflows int for",c))
    if(t$destinationtype[t$colname==c] == "smallint" & m> 32767)
      stop(paste("Number overflows smallint for",c))
    if(t$destinationtype[t$colname==c] == "tinyint" & m> 255)
      stop(paste("Number overflows tinyint for",c))
    t$importtype[t$colname==c]<-t$destinationtype[t$colname==c]
  }
  #Handle dates
  for(c in t$colname[t$destinationtype %in% c("date","datetime2")& t$importtype=="character"]){
    if(any(!is.na(fd[,c])& 
           is.na(as.Date(fd[,c]))))
      stop(paste("Conversion to date failed for",c))
    fd[,c]<-as.Date(fd[,c])
    t$importtype[t$colname==c]<-t$destinationtype[t$colname==c]
  }
  #Handle bits
  for(c in t$colname[t$destinationtype %in% c("bit")& t$importtype!="bit"]){
    newval<-text_to_bit(fd[,c])
    if(any(!is.na(fd[,c])&  !fd[,c] %in% c("",":") &
           is.na(newval))){
      summary(factor(fd[,c]))
      View(fd[!is.na(fd[,c])&is.na(newval),c])
      stop(paste("Conversion to bit failed for",c))
    }
    fd[,c]<-newval
    t$importtype[t$colname==c]<-t$destinationtype[t$colname==c]
  }
  
  handled_destination<-c("varchar","nvarchar","decimal","int","bigint","smallint","tinyint","date","datetime2","bit")
  if(any(!t$destinationtype %in% handled_destination)){
    stop(paste("Unknown destination type ",t$destinationtype[!t$destinationtype %in% handled_destination]))
  }
  print(c("File",file.list[f],"Error Checking Stop", format(Sys.time(), "%c")))
}

```

## 4) Upload from R to SQL server

This section uploads the monthly delta files to SQL server. Each file
takes between a half hour and an hour to upload per file. Separating
this from step 3 is less efficient, but means uploads will not be
interrupted mid process by the need to fix column mismatches.


First upload attempt, before the NaN errors, was 100k rows in a about 12
minutes, or about 2 hours per file.\
I started hitting OBDC upload errors with no details, so I was planning
to shrink down the interval. After discover the NaN import bug, where
character columns exclusively NAN or NULL were treated as "not a number"
numerics, I accidentally restarted runs with a 25 row interval. I did
switch to 25k when rerunning last night, but first it failed for a new
reason. "Sat Jun 21 21:56:12 2025"
`Error: nanodbc/nanodbc.cpp:4616: 08S01: [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionWrite (send()).  [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation.`
`Error occurs again on file 13 Error: nanodbc/nanodbc.cpp:4616: 08S01: [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionWrite (send()).  [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation.`
To my surprise, when restarting with a 25k interval, the first file
completed from 800 to 829, or about 30 minutes per.


#### Past upload time log:

-   2025-08-19 upload: 3.13M rows. Started 07:23 finished 11:33.
-   2025-11-26 upload; 3,50M rows, two batches: 23:23 to 00:18 & 09:08
    to 12:43

```{r uploadtoSQL}
for (f in 1:length(file.list)){
  load(file.path(path,deltadir,file.list[f]))
  fd<-as.data.frame(fd)
  print(c("File",file.list[f],"Error Checking Start", format(Sys.time(), "%c")))
  vmcon <- dbConnect(odbc(),
                     Driver = "SQL Server",
                     Server = "vmsqldiig.database.windows.net",
                     Database = "CSIS360",
                     UID = login,
                     PWD =pwd)
  
    interval<-25000
    rowcount<-nrow(fd)
    #Roughy 12 minutes per 100k interval, so two hours per file.
    for (r in 0:ceiling(rowcount/interval)){  #reset to 0 when not recovering from a crash
      start<-1+r*interval
      end<-(r+1)*interval
      if(start>rowcount) {break}
      if(end>rowcount) {end<-rowcount}
      fd_filtered<-fd[start:end,]
      # ctu<-fd_filtered$contract_transaction_unique_key
      # t %>% filter(importtype %in% c("numeric") & !t$destinationtype %in% c("decimal","int","bigint","smallint","tinyint"))
      # fd_filtered[,t$importtype %in% c("numeric") &
      #               !t$destinationtype %in% c("decimal","int","bigint","smallint","tinyint")]<-NA
      # # fd_filtered[,t$destinationtype %in% c("decimal","int","bigint","smallint","tinyint")]<-NA #%in% c("decidecimal","int","bigint","smallint","tinyint")]
      # summary(factor(fd$type_of_idc))
      
      print(c("Upload Current Start:",start,"Next Start:",end+1, nrow(fd_filtered),format(Sys.time(), "%c")))
      dbAppendTable(conn = vmcon, 
                    name = SQL('"ETL"."FPDSdelta"'), 
                    value = fd_filtered)  ## x is any data frame
      #https://stackoverflow.com/questions/66864660/r-dbi-sql-server-dbwritetable-truncates-rows-field-types-parameter-does-not-w
    }
  #Note, transactions will not commit until you disconnect! Whether or not
  #the computer maintains this connection in the meantime doesn't matter,
  #formal disconnection is required.
  dbDisconnect(vmcon)
}
```
